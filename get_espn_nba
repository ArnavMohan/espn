#!/usr/bin/python

import urllib
import urlparse
import urllib2
import os
import sys
import datetime
import argparse
#import pdfkit

from HTMLParser import HTMLParser


def m2num(m):
  return{
    'January' : 1,
    'February' : 2,
    'March' : 3,
    'April' : 4,
    'May' : 5,
    'June' : 6,
    'July' : 7,
    'August' : 8,
    'September' : 9,
    'October' : 10,
    'November' : 11,
    'December' : 12,
  }[m]

lastday = datetime.date.today()
seasons = {}
seasons[2012] = {}
seasons[2012][0] = datetime.date(2012,10,30)
seasons[2012][1] = datetime.date(2013,6,20)
seasons[2013] = {}
seasons[2013][0] = datetime.date(2013,10,29)
seasons[2013][1] = datetime.date(2014,6,16)
seasons[2014] = {}
seasons[2014][0] = datetime.date(2014,10,27)
seasons[2014][1] = datetime.date(2015,6,17)
seasons[2015] = {}
seasons[2015][0] = datetime.date(2015,10,26)
seasons[2015][1] = datetime.date(2016,6,20)
seasons[2016] = {}
seasons[2016][0] = datetime.date(2016,10,24)
seasons[2016][1] = datetime.date(2017,6,13)
seasons[2017] = {}
seasons[2017][0] = datetime.date(2017,10,16)
seasons[2017][1] = lastday

changes = {}
changes[2013] = {}
changes[2013]['Hornets'] = 'Pelicans'
changes[2014] = {}
changes[2014]['Bobcats'] = 'Hornets'

ignore_teams = ['USA','Stephen','All-Stars','World']
def ignore_team(team):
  for t in ignore_teams:
    if t in team:
      return True
  return False

def name_change(n,d):
  for s in sorted(seasons.keys()):
    if d >= seasons[s][0] and d <= seasons[s][1]:
      break
  for y in sorted(changes.keys()):
    if s < y:
      if n in changes[y]:
        return changes[y][n]
  return n

def valid_date(d):
  for y in seasons:
    if d >= seasons[y][0] and d <= seasons[y][1]:
      return true
  return false

def valid_date_parse(s):
  try:
    return datetime.datetime.strptime(s,"%Y-%m-%d").date()
  except ValueError:
    msg = "Not a valid date: '{0}'.".format(s)
    raise argparse.ArgumentTypeError(msg)

parser = argparse.ArgumentParser()
parser.add_argument("-D", "--dir", help="Output directory", default="./")
parser.add_argument("-S", "--start", help="The first date to start from - format YYYY-MM-DD", type=valid_date_parse, default=seasons[2012][0])
parser.add_argument("-A", "--append", help="Append to the previous file", action='store_true')
args = parser.parse_args()


###################################################################
# AbortFeed Exception
###################################################################
class AbortFeed(Exception):
  pass

###################################################################
# Generic HTMLParser
###################################################################
class MyHTMLParser(HTMLParser):
  html = ""
  url = ""
  maxattempt = 1
  MAXA = 10

  def fetch_url(self,url):
    self.url = url
    attempt = 1
    dourl = True
    while attempt <= self.MAXA and dourl:
      dourl = False
      try:
        request = urllib2.Request(url)
        response = urllib2.urlopen(request)
      except urllib2.URLError, e:
        attempt += 1
        dourl = True
      except urllib2.HTTPError, e:
        attempt += 1
        dourl = True
    if dourl:
      print "Aborting due to multiple exceptions on " + url
      sys.exit(1)
    if attempt > self.maxattempt:
      self.maxattempt = attempt
    if attempt > 1:
      print "Had {} attempts for {}".format(attempt,url)
    link = response.geturl()
    self.html = response.read()
    response.close()
  
  def my_feed(self):
    try:
      self.feed(self.html)
    except UnicodeDecodeError, e:
      pass
    except AbortFeed, e:
      pass

  def parse_url(self,url):
    self.fetch_url(url)
    urlinfo = urlparse.urlparse(url)
    self.url_base = urlinfo.scheme + "://" + urlinfo.hostname + "/"
    self.tags = {}
    self.attrs = {}
    self.my_feed()

  def handle_starttag(self,tag,attrs):
    self.tags[tag] = True
    self.attrs[tag] = attrs
    self.curattr = attrs
    self.curtag = tag

  def handle_endtag(self,tag):
    self.tags[tag] = False

  def intag(self,tag):
    if tag in self.tags:
      return self.tags[tag]
    return False

  def print_html(self):
    print "URL:" + self.url
    print self.html

  def print_tags(self,tag=""):
    if tag:
      if not tag in self.tags:
        return
      tlist = [tag]
    else:
      tlist = self.tags.keys()

    for t in tlist:
      print "Tag=" + t
      for attr in self.attrs[t]:
        print " {}:{}".format(attr[0],attr[1])

      
###################################################################
# Game Team Stats
###################################################################
class GameParser(MyHTMLParser):

  def intitle(self):
    return self.intag("title")

  def instat(self):
    return self.intag("td") and self.intag("tr") and self.intag("tbody") and self.intag("table") and self.intag("div") and self.intag("article")

  def parse_url(self,url):

    self.nogame = False

    self.parse_order = [
                  "FG Made-Attempted",
                  "3PT Made-Attempted",
                  "FT Made-Attempted",
                  "Total Rebounds",
                  "Offensive Rebounds",
                  "Defensive Rebounds",
                  "Assists",
                  "Steals",
                  "Blocks",
                  "Total Turnovers",
                  "Points Off Turnovers",
                  "Fast Break Points",
                  "Points in Paint",
                  "Personal Fouls",
                  "Technical Fouls",
                  "Flagrant Fouls",
                 ]
    self.parse_fields = {}
    for o in self.parse_order:
      self.parse_fields[o] = {}
      self.parse_fields[o][0] = {}
    for o in ["FG Made-Attempted","3PT Made-Attempted","FT Made-Attempted"]:
      self.parse_fields[o] = {}
      self.parse_fields[o][0] = {}
      self.parse_fields[o][1] = {}

    self.curop = 0
    self.curopcnt = 0

    self.fetch_url(url)
    urlinfo = urlparse.urlparse(url)
    self.url_base = urlinfo.scheme + "://" + urlinfo.hostname + "/"
    self.tags = {}
    self.attrs = {}

    if "NBA Basketball Scores - NBA Scoreboard - ESPN" in self.html:
      self.nogame = True
      return
    if "No Team Stats Available" in self.html:
      self.nogame = True
      return

    self.my_feed()

  def parse_data(self,d):

    if self.curopcnt == 0:
      if self.parse_order[self.curop] in d:
        self.curopcnt = 1
      return
    op = self.parse_order[self.curop]
    if self.curopcnt == 2:
      tm = 'home'
      self.curopcnt = 0
      self.curop += 1
    else:
      tm = 'away'
      self.curopcnt = 2

    if ':' in d:
      try:
        a,b = d.split(':')
        aint = int(a.strip())
        bint = int(b.strip())
        aint = aint*60 + bint
      except ValueError:
        self.nogame = True
        print "value eror ", d
        raise AbortFeed
        return

      self.parse_fields[op][0][tm] = aint
      return

    if '-' in d:
      try:
        a,b = d.split('-')
        aint = int(a.strip())
        bint = int(b.strip())
      except ValueError:
        self.nogame = True
        print "value eror ", d
        raise AbortFeed
        return

      (self.parse_fields[op][0][tm],self.parse_fields[op][1][tm]) = (aint,bint)
      return

    else:
      try:
        aint = int(d.strip())
      except ValueError:
        self.nogame = True
        print "value eror ", d
        raise AbortFeed
        return

      self.parse_fields[op][0][tm] = aint
      return


  def handle_data(self,data):
    if self.nogame:
      return
    if self.intitle():
      away,rest = data.split("vs.")
      home,rest = rest.split("- Team Statistics -")
      day,rest = rest.split('-')
      #print "Title: " + data
      self.home = home.strip()
      self.away = away.strip()
      if ignore_team(self.home) or ignore_team(self.away):
        self.nogame = True
        raise AbortFeed
        return
      md,y = day.strip().split(',')
      m,d = md.split()

      y = int(y.strip())
      m = m2num(m.strip())
      d = int(d.strip())
      self.date = datetime.date(y,m,d)
      self.home = name_change(self.home,self.date)
      self.away = name_change(self.away,self.date)
      #print self.home, self.away, self.date


    if self.instat():
      self.parse_data(data)
      if self.curop >= len(self.parse_order):
        raise AbortFeed
        return

  def print_game(self,id,fp):
    if self.nogame:
      return
    score = {}
    score['away'] = 2 * self.parse_fields["FG Made-Attempted"][0]['away'] + self.parse_fields["3PT Made-Attempted"][0]['away'] + self.parse_fields["FT Made-Attempted"][0]['away']
    score['home'] = 2 * self.parse_fields["FG Made-Attempted"][0]['home'] + self.parse_fields["3PT Made-Attempted"][0]['home'] + self.parse_fields["FT Made-Attempted"][0]['home']
    row = id + ','
    row += str(self.date) + ','
    row += self.home + ','
    row += self.away + ','
    row +=  self.home if (score['home'] > score['away']) else self.away
    row +=  ','
    row +=  str(score['home']) + ',' + str(score['away']) + ','

    for f in self.parse_order:
      if len(self.parse_fields[f]) == 1:
        row +=  str(self.parse_fields[f][0]['home']) + ',' + str(self.parse_fields[f][0]['away']) + ','
      else:
        row +=  str(self.parse_fields[f][0]['home']) + ',' + str(self.parse_fields[f][1]['home']) + ',' + str(self.parse_fields[f][0]['away']) + ',' + str(self.parse_fields[f][1]['away']) + ','

    fp.write(row + '\n')

      
###################################################################
# Day Parser
###################################################################
class DayParser(MyHTMLParser):

  def inscript(self):
    return self.intag("script")

  def parse_url(self,url):
    self.fetch_url(url)
    urlinfo = urlparse.urlparse(url)
    self.url_base = urlinfo.scheme + "://" + urlinfo.hostname + "/"
    self.tags = {}
    self.attrs = {}

    self.games = []
    self.done = False
    if "No games on this date" in self.html:
      return

    self.my_feed()

  def handle_data(self,data):
    if self.done:
      return
    if self.inscript():
      if data.startswith("window.espn.scoreboardData"):
        parts = data.split("nba/game?gameId=")
#        print len(parts)
        for i in range(1,len(parts)):
          self.games.append(parts[i][:9])
        self.done = True
        raise AbortFeed

      
  def get_games(self):
    return self.games


      
###################################################################
# Main Code
###################################################################
if args.append:
  fop = 'a'
else:
  fop = 'w'
f = open(os.path.join(args.dir,'nbadata.csv'),fop)

basedayurl = "http://www.espn.com/nba/scoreboard/_/date/"
basegameurl = "http://www.espn.com/nba/matchup?gameId="
if not args.append:
  f.write("id,Date,Home,Away,Winner,Score_Home,Score_Away,FGM_Home,FGA_Home,FGM_Away,FGA_Away,3PM_Home,3PA_Home,3PM_Away,3PA_Away,FTM_Home,FTA_Home,FTM_Away,FTA_Away,REB_Home,REB_Away,OR_Home,OR_Away,DR_Home,DR_Away,ASS_Home,ASS_Away,ST_Home,ST_Away,BL_Home,BL_Away,TO_Home,TO_Away,POT_Home,POT_Away,FBP_Home,FBP_Away,PIP_Home,PIP_Away,PF_Home,PF_Away,TF_Home,TF_Away,FF_Home,FF_Away" + '\n')

for s in sorted(seasons.keys()):
  if args.start <= seasons[s][1]:   # specified start date is before the end of season 
    dd = seasons[s][0]
    if args.start >= dd:            # check is starting in middle of season
      dd = args.start
    while dd <= seasons[s][1]:
      print dd
      dayurl = basedayurl + str(dd.year) + '{:02d}'.format(dd.month) + '{:02d}'.format(dd.day)
      day = DayParser()
      day.parse_url(dayurl)
      for g in day.get_games():
        gameurl = basegameurl + g
        game = GameParser()
        print "  ", g, gameurl
        game.parse_url(gameurl)
        game.print_game(g,f)
    
      dd = dd + datetime.timedelta(1)


f.close()

#baseurl = "http://www.espn.com/nba/matchup?gameId="
#for i in range(400277700,401030000):


